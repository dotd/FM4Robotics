# Towards A Unified Agent with Foundation Models

- Main question: Can we leverage the capabilities of
  Vision Language Models (VLM) to design efficient & general RL agents?
- They design a framework that puts language at the core of an RL robotic agent,
  - particularly in the context of learning from scratch.
- They show that LLMs and VLMs can tackle fundamental problems in RL such as:
    1. Efficiently exploring sparse-reward environments
    2. Re-using collected data to bootstrap the learning of new tasks
    3. Scheduling learned skills to solve novel tasks
    4. Learning from observation of expert agents.